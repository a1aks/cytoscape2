<?xml version="1.0"?>
<document>
  <body>

  <section name="About this Document">
    <p>
	    This document describes functionality that will be built in Phase 2 of the 
	    Cytoscape Data Services project.  The focus of Phase 2 is to A)  create a
	    local mirror of the GRID database and create an XML-based web service to the GRID data; and 
	    B) stabilize some of the Phase 1 code, e.g. evaluate a common logging framework,
	    create a centralized properties manager, and create a consistent UI for handling
	    data service exceptions.
    </p>
  </section>
  <section name="Feature #1:  Create an XML-based web service to GRID data">
    <P>
	    Currently, the Cytoscape data services framework connects directly to the 
	    GRID mySQL database via JDBC.  After considering this further, the GRID team 
	    would prefer that we create a local mirror of the entire GRID database, and 
	    access GRID data from here instead.  In a way, this represents a significant
	    shift in focus for the data services project.  Our original goal was to connect
	    to existing databases, and minimize the amount of time spent maintaining
	    and support the actual data sources.  However, our data services API is only
	    as good as the data sources we connect to, and we therefore need to plan for 
	    constant up-time, backward compatibility, and hosting of database mirrors.
    </P>
    <P>
	    One simple option is to simply copy the entire GRID mySQL databse, and leave the 
	    mySQL TCP connections open to the world (this is essentially how GRID currently
	    operates).  A number of people have advised against doing this, as it leaves
	    the entire database open to a number of known security vulnerabilities.  We are
	    therefore going with a second option, which is to create an XML-based web service
	    for accessing GRID data.  The proposed service is desribed below.
    </P>
    <subsection name="The CBio Data Service">
    <P>
	    The CBio data service will provide access to GRID interaction data via a 
	    simple URI API.  Clients will send requests via URL parameters and retrieve
	    XML-encoded responses.  In the future, this service may also provide access to
	    non-GRID data, and may also evolve to support other protocols, such as XML-RPC
	    and SOAP.  
    </P>
    <P>
	    Requests to the CBio Data Service are formed by specifying URL parameters.  The API
	    will be based roughly on the 
	    <A HREF="http://www.ncbi.nlm.nih.gov:80/entrez/query/static/linking.html">NCBI URL API</A>,
            and will initially support five parameters:
	    <UL>
		    <LI>cmd:  Indicates the command to execute.  During phase 2, the only supported
			    commands will be "retrieve_interactions" and "retrieve_go".
		    </LI>
		    <LI>db:  Indicates the requested database.  During phase 2, the only supported
			    database option will be "grid".
		    </LI>
		    <LI>uid:  Indicates a unique identifier.  For example, when referencing GRID data,
			    one would use an ORF Name as a UID.  Users are free to specify multiple UIDs.
		    </LI>
		    <LI>format:  Indicates the format of returned results.  In Phase 2, the only
			    supported formats will be "psi" and "go".  "psi" indicates the Proteomics 
			    Standards Initiative - Molecular Interactions (PSI-MI) XML Schema format.  
			    Details are available from the 
			    <A HREF="http://psidev.sourceforge.net/"> PSI-Dev</A> web site.  "go"
			    indicates an ad-hoc XML format for representing GO Terms.  Exact format 
			    is still under consideration.
	            </LI>
		    <LI>version:  Indicates the version of the dataservice protocol.  Must be
			    specified.  In Phase 2, the only supported version will be "1.0".
		    </LI>
	    </UL>
	    For example, the following query requests all interactions for ORF:  YDR362C, from
	    GRID, formatted in PSI-MI:
    </P>
	    <source>
<![CDATA[
http://cbio.mskcc.org/path_db/data_service?cmd=retrieve&db=grid&uid=YDR362C&format=psi&version=1.0
]]>
	    </source> 
	    <P>If no arguments are specified, the data service will return a human readable
		    description of the protocol.
	    </P>
    <P>
	    The data service response will be a well-formed XML document, containing the results
	    of the query.  The format of the response document will be based on the input "format"
	    argument.  The HTTP response will also include an HTTP Header, named "DS-STATUS", 
	    indicating the status of the request.  The value of the header will consist of
	    a status code number, followed by a human-readable status message.  Status code
	    will be mostly borrowed from the Distributed Annotation System 
	    (<A HREF="http://www.biodas.org">DAS</A>) specification. 
    </P>
    <P>
	    Status Codes:
	    <UL>
		    <LI>200:  OK, data follows</LI>
		    <LI>400:  Bad Command (command not recognized)</LI>
		    <LI>401:  Bad Data Source (Data source unknown)</LI>
		    <LI>450:  Unknown UID</LI>
		    <LI>451:  Specified format is not supported</LI>
	    </UL>
	    In the event of an error, the data service will return both a DS-STATUS Header Code
	    message, and a short XML document containing the error code.  In effect, this means that
	    we will be placing the same error code in two locations.  This is slightly redundant, but
	    it will make is easier for both software applications and human users to easily identify 
	    the errors.  For example, a user who is testing the protocol via a web browser can
	    easily identify errors by inspecting the returned xml document.
	    </P>
	    <P>
	    A sample error code document is shown below:
            </P>
	    <source>
<![CDATA[
<error>
	<error_code>400</error_code>
	<error_msg>Bad Command (command not recognized)</error_msg>
</error>
]]>
	    </source>
    </subsection>
    <subsection name="Architectural Overview">
    	<P>
	Figure 1 provides an architectural overview of the CBio Data Service.
	<CENTER>
		<IMG SRC="images/dataservices.jpg"/>
	</CENTER>
	</P>
	<P>
		This is a fairly typical 3-tier web application.  The first tier consists of
		clients, including web browsers and software applications, such as Cytoscape.
		The second tier consists of Java servlet/controller code for accessing the
		database.  All calls to the data service will go through a central controller
		object, which will centralize all exception handling and debugging.  The 
		database classes will be responsible for connecting to the database, and wrapping
		the results in XML.  The third tier consists of the MySQL database.  The items
		drawn with a dotted line indicate future databases that may be added to the 
		data service in the future.

	</P>
    </subsection>
    <subsection name="Specific Task List">
	<P>
	The following represents a high-level list of tasks that will be included in Feature #1.
	<UL>
		<LI>Local installation of mySQL, Tomcat, GRID database on Mac OS X for development.</LI>
		<LI>Create Servlet to support URL API.</LI>
		<LI>Create adapter for transforming GRID data into PSI format.</LI>
		<LI>Update Cytoscape data services code to use new CBio Data Service.</LI>
		<LI>CBio installation, including Tomcat, GRID mirror, etc.</LI>
		<LI>Create auto-tester to automatically verify web service uptime.</LI>
		<LI>Verify backup system for mySQL.</LI>
	</UL>
	</P>
    </subsection>
  </section>
  <section name="Feature #2:  Stabilizing Data Services Code">
	  <P>
		This section consists of several items that didn't make it into Phase 1, 
		and will help stabilize the code base.  The list includes:
		<UL>
			<LI>Evaluate a common logging system for all data services code.</LI>
			<LI>Currently, all database urls and passwords are hard-codes.
				We need to move these into a centralized properties manager,
				and / or a centralized configuration file.
			</LI>
			<LI>Currently, if a data service times out or throws an exception,
				the code simply prints a stack trace to the screen.  We need
				to clean this up, so that a window is presented to the user,
				and the window includes some type of intelligible error message.
			</LI>
			<LI>Integrate all JUnit test into Maven for automatic site generation.
			</LI>
		</UL>
	  </P>
  </section>
  <section name="Schedule">
	  <P>
		  Phase 2 should take approximately three weeks.  This assumes approximately two weeks
		  of development time, and one week to get everything installed and running
		  smoothly on CBio.  Assuming we nail down user functionality by Monday, 
		  May 20 we can start real coding on Tuesday, May 21.  Phase 1 should therefore 
		  be complete by Tuesday, June 11.  
	  </P>
  </section>
</body>
</document>
